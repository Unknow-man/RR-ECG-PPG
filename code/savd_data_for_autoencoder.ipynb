{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "data = pickle.load(open('../data/60_second_data1.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hrvanalysis import get_time_domain_features\n",
    "from joblib import Parallel,delayed\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(a):\n",
    "    features = []\n",
    "    ecg_rr = a[:,-1]\n",
    "    if len(ecg_rr[np.isnan(ecg_rr)])>40:\n",
    "        return [],[],[],[],[],[]\n",
    "    m = np.mean(ecg_rr[ecg_rr>0])\n",
    "    s = np.std(ecg_rr[ecg_rr>0])\n",
    "    ecg_rr[np.isnan(ecg_rr)] = 0\n",
    "    if len(ecg_rr)<60:\n",
    "        return [],[],[],[],[],[]\n",
    "    y = []\n",
    "    X = []\n",
    "    ecg = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    quals = []\n",
    "    for i in [-2]:\n",
    "        ppg_rr = a[:,i]\n",
    "        ppg_qual = a[:,i-4]\n",
    "#         index = ppg_rr>0\n",
    "#         ppg_qual = ppg_qual[index]\n",
    "#         ppg_rr = ppg_rr[index]\n",
    "        index = np.isnan(ppg_rr)\n",
    "        index1 = ~np.isnan(ppg_rr)\n",
    "        if len(ppg_rr[index1])<10:\n",
    "            continue\n",
    "        ppg_qual[index] = -1\n",
    "        ppg_rr[index] = np.nanmean(ppg_rr)\n",
    "        y.append(ppg_rr.reshape(1,60,1))\n",
    "        tmp = a[:,np.array([-2,-6,1])].reshape(1,60,3)\n",
    "        tmp[np.isnan(tmp)] = 0\n",
    "        tmp[tmp==0] = 0\n",
    "        X.append(tmp)\n",
    "        means.append(m)\n",
    "        stds.append(s)\n",
    "        ecg.append(ecg_rr.reshape(1,60,1))\n",
    "        quals.append(ppg_qual.reshape(1,60,1))\n",
    "#         for j in np.linspace(0,.9,20):\n",
    "#             index = ppg_qual>j\n",
    "#             ppg_qual = ppg_qual[index]\n",
    "#             ppg_rr = ppg_rr[index]\n",
    "#             if len(ppg_rr)<10:\n",
    "#                 continue\n",
    "#             f = list(get_time_domain_features(ppg_rr).values())\n",
    "#             f1 = list(get_time_domain_features(ecg_rr).values())\n",
    "#             q = [np.percentile(ppg_qual,20),np.median(ppg_qual),len(ppg_rr)/60]\n",
    "#             features.append(np.array(f1+f+q))\n",
    "    return X,y,ecg,means,stds,quals\n",
    "\n",
    "df_col = Parallel(n_jobs=10,verbose=1)(delayed(get_data)(a) for a in data if len(a[~np.isnan(a[:,-1]),-1])>20)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,ecg,means,stds,quals = [],[],[],[],[],[]\n",
    "for a in df_col:\n",
    "    if len(a[0])==0:\n",
    "        continue\n",
    "    X.extend(a[0])\n",
    "    y.extend(a[1])\n",
    "    ecg.extend(a[2])\n",
    "    means.extend(a[3])\n",
    "    stds.extend(a[4])\n",
    "    quals.extend(a[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.concatenate(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,ecg,means,stds,quals = np.concatenate(X),np.concatenate(y).reshape(-1,60),np.concatenate(ecg).reshape(-1,60),\\\n",
    "np.array(means).reshape(-1,1),np.array(stds).reshape(-1,1),np.concatenate(quals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, RepeatVector,Bidirectional,Multiply,multiply,Permute\n",
    "from keras.layers import TimeDistributed,Dense,Flatten,Reshape,Lambda,Activation,GRU\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras import metrics,losses\n",
    "import tensorflow as tf\n",
    "# import tensorflow_probability as tfp\n",
    "X_train, X_test, y_train, y_test,ecg_train, \\\n",
    "ecg_test,means_train,means_test,stds_train,stds_test, \\\n",
    "quals_train,quals_test= train_test_split(\n",
    "    X, y,ecg,means,stds,quals, test_size=0.33, random_state=42)\n",
    "X_train, X_val, y_train, y_val,means_train,means_val,stds_train,stds_val = train_test_split(\n",
    "    X_train, y_train,means_train,stds_train, test_size=0.2, random_state=42)\n",
    "print(X.shape,y.shape,y_val.shape,means_val.shape,stds_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           (None, 60, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_37 (Bidirectional (None, 60, 120)      23040       input_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_36 (TimeDistri (None, 60, 1)        121         bidirectional_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_38 (Flatten)            (None, 60)           0           time_distributed_36[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 60)           0           flatten_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_12 (RepeatVector) (None, 120, 60)      0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_13 (Permute)            (None, 60, 120)      0           repeat_vector_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_20 (Multiply)          (None, 60, 120)      0           permute_13[0][0]                 \n",
      "                                                                 bidirectional_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_37 (TimeDistri (None, 60, 10)       1210        multiply_20[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_39 (Flatten)            (None, 600)          0           time_distributed_37[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "sequence (Dense)                (None, 60)           36060       flatten_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mean (Lambda)                   (None, 1)            0           sequence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "std (Lambda)                    (None, 1)            0           sequence[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 60,431\n",
      "Trainable params: 60,431\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def output_of_lambda(input_shape):\n",
    "    return (input_shape[0], 1)\n",
    "\n",
    "def mean(x):\n",
    "    return K.mean(x, axis=1, keepdims=True)\n",
    "\n",
    "def output_of_lambda1(input_shape):\n",
    "    return (input_shape[0], 1)\n",
    "\n",
    "def mean1(x):\n",
    "    return K.std(x, axis=1, keepdims=True)\n",
    "\n",
    "timesteps = 60\n",
    "input_dim = 3\n",
    "latent_dim = 20\n",
    "output_dim = 1\n",
    "n = 1\n",
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "# inputs2 = Reshape((1,1))(inputs1)\n",
    "encoded = Bidirectional(GRU(60,return_sequences=True,activation='relu',go_backwards=True))(inputs)\n",
    "# encoded = LSTM(output_dim,return_sequences=True,activation='sigmoid')(encoded)\n",
    "att = TimeDistributed(Dense(1,activation='relu'))(encoded)\n",
    "\n",
    "att = Flatten()(att)\n",
    "att = Activation(activation=\"softmax\")(att)\n",
    "att = RepeatVector(120)(att)\n",
    "att = Permute((2,1))(att)\n",
    "mer = multiply([att, encoded])\n",
    "\n",
    "encoded = TimeDistributed(Dense(10,activation='relu'))(mer)\n",
    "encoded = Flatten()(encoded)\n",
    "# encoded = Dense(10,activation='relu',name='sequence1')(encoded)\n",
    "encoded = Dense(60,activation='relu',name='sequence')(encoded)\n",
    "# encoded = Reshape((60),name='sequence')(encoded)\n",
    "# encoded_std = K.std(encoded,axis=1)\n",
    "decoded = Lambda(mean, output_shape=output_of_lambda,name='mean')(encoded)\n",
    "decoded1 = Lambda(mean1, output_shape=output_of_lambda1,name='std')(encoded)\n",
    "# decoded = LSTM(output_dim*60, return_sequences=True)(decoded)\n",
    "# decoded = LSTM(output_dim*3, return_sequences=True)(decoded)\n",
    "# decoded = LSTM(output_dim, return_sequences=True)(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs=[inputs], outputs=[encoded,decoded,decoded1])\n",
    "# encoder = Model(inputs, encoded)\n",
    "losses = {\n",
    "    \"std\":\"logcosh\",\n",
    "    \"mean\": \"logcosh\",\n",
    "    \"sequence\": \"mae\"\n",
    "}\n",
    "lossWeights = {\"mean\": 1, \"sequence\": 1,\"std\":1}\n",
    "# initialize the optimizer and compile the model\n",
    "\n",
    "sequence_autoencoder.compile(optimizer='adam',loss=losses, loss_weights=lossWeights)\n",
    "\n",
    "sequence_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14228 samples, validate on 3557 samples\n",
      "Epoch 1/300\n",
      "14228/14228 [==============================] - 6s 412us/step - loss: 1561.4482 - sequence_loss: 752.2248 - mean_loss: 772.4379 - std_loss: 35.1775 - val_loss: 1490.8868 - val_sequence_loss: 727.0180 - val_mean_loss: 741.1714 - val_std_loss: 20.2614\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1490.88676, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 2/300\n",
      "14228/14228 [==============================] - 14s 999us/step - loss: 1235.8447 - sequence_loss: 589.9869 - mean_loss: 591.7291 - std_loss: 44.0152 - val_loss: 647.9706 - val_sequence_loss: 304.4780 - val_mean_loss: 218.6539 - val_std_loss: 121.4944\n",
      "\n",
      "Epoch 00002: val_loss improved from 1490.88676 to 647.97059, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 3/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 437.7117 - sequence_loss: 229.9534 - mean_loss: 132.8577 - std_loss: 72.7345 - val_loss: 305.5823 - val_sequence_loss: 186.3163 - val_mean_loss: 96.3773 - val_std_loss: 20.3200\n",
      "\n",
      "Epoch 00003: val_loss improved from 647.97059 to 305.58234, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 4/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 285.5954 - sequence_loss: 175.2453 - mean_loss: 90.9847 - std_loss: 19.1136 - val_loss: 273.4284 - val_sequence_loss: 171.5606 - val_mean_loss: 83.1317 - val_std_loss: 17.3389\n",
      "\n",
      "Epoch 00004: val_loss improved from 305.58234 to 273.42842, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 5/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 272.9436 - sequence_loss: 168.9127 - mean_loss: 85.7204 - std_loss: 17.9937 - val_loss: 271.1860 - val_sequence_loss: 169.3371 - val_mean_loss: 82.6856 - val_std_loss: 17.5669\n",
      "\n",
      "Epoch 00005: val_loss improved from 273.42842 to 271.18601, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 6/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 268.3982 - sequence_loss: 166.4852 - mean_loss: 84.4323 - std_loss: 17.9057 - val_loss: 261.5636 - val_sequence_loss: 164.9354 - val_mean_loss: 78.0548 - val_std_loss: 17.3889\n",
      "\n",
      "Epoch 00006: val_loss improved from 271.18601 to 261.56362, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 7/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 264.8357 - sequence_loss: 164.0863 - mean_loss: 82.6645 - std_loss: 17.9384 - val_loss: 262.5543 - val_sequence_loss: 164.6076 - val_mean_loss: 78.5278 - val_std_loss: 17.4949\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 261.56362\n",
      "Epoch 8/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 264.5200 - sequence_loss: 163.8606 - mean_loss: 82.7546 - std_loss: 18.0207 - val_loss: 256.6589 - val_sequence_loss: 162.3045 - val_mean_loss: 75.8661 - val_std_loss: 17.3152\n",
      "\n",
      "Epoch 00008: val_loss improved from 261.56362 to 256.65894, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 9/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 259.4600 - sequence_loss: 161.6196 - mean_loss: 80.0671 - std_loss: 17.8289 - val_loss: 254.1228 - val_sequence_loss: 161.4918 - val_mean_loss: 74.0941 - val_std_loss: 17.2652\n",
      "\n",
      "Epoch 00009: val_loss improved from 256.65894 to 254.12279, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 10/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 257.8921 - sequence_loss: 160.9640 - mean_loss: 78.9832 - std_loss: 17.8883 - val_loss: 252.4596 - val_sequence_loss: 161.0071 - val_mean_loss: 73.2168 - val_std_loss: 17.2387\n",
      "\n",
      "Epoch 00010: val_loss improved from 254.12279 to 252.45962, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 11/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 256.5402 - sequence_loss: 160.3143 - mean_loss: 78.3086 - std_loss: 17.8305 - val_loss: 251.7393 - val_sequence_loss: 160.4591 - val_mean_loss: 73.2826 - val_std_loss: 17.1548\n",
      "\n",
      "Epoch 00011: val_loss improved from 252.45962 to 251.73932, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 12/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 255.8825 - sequence_loss: 160.1678 - mean_loss: 78.0214 - std_loss: 17.9409 - val_loss: 252.8315 - val_sequence_loss: 161.1165 - val_mean_loss: 73.9447 - val_std_loss: 17.0727\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 251.73932\n",
      "Epoch 13/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 257.3553 - sequence_loss: 161.0088 - mean_loss: 78.3796 - std_loss: 17.9199 - val_loss: 251.3663 - val_sequence_loss: 160.1776 - val_mean_loss: 72.8474 - val_std_loss: 17.3163\n",
      "\n",
      "Epoch 00013: val_loss improved from 251.73932 to 251.36629, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 14/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 253.7363 - sequence_loss: 159.4402 - mean_loss: 76.7241 - std_loss: 17.8582 - val_loss: 249.9298 - val_sequence_loss: 159.6037 - val_mean_loss: 71.8305 - val_std_loss: 17.0408\n",
      "\n",
      "Epoch 00014: val_loss improved from 251.36629 to 249.92983, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 15/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 253.1495 - sequence_loss: 158.8539 - mean_loss: 76.3161 - std_loss: 17.8737 - val_loss: 248.6016 - val_sequence_loss: 159.1226 - val_mean_loss: 71.5501 - val_std_loss: 17.0576\n",
      "\n",
      "Epoch 00015: val_loss improved from 249.92983 to 248.60157, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 16/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 252.0022 - sequence_loss: 158.3731 - mean_loss: 75.8946 - std_loss: 17.8438 - val_loss: 246.4763 - val_sequence_loss: 158.1738 - val_mean_loss: 70.5536 - val_std_loss: 17.2256\n",
      "\n",
      "Epoch 00016: val_loss improved from 248.60157 to 246.47634, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 17/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 250.7472 - sequence_loss: 157.9481 - mean_loss: 75.0566 - std_loss: 17.8640 - val_loss: 245.1351 - val_sequence_loss: 157.6436 - val_mean_loss: 69.5123 - val_std_loss: 17.0057\n",
      "\n",
      "Epoch 00017: val_loss improved from 246.47634 to 245.13512, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 18/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 250.9207 - sequence_loss: 157.8765 - mean_loss: 75.1506 - std_loss: 17.8191 - val_loss: 245.8764 - val_sequence_loss: 157.7101 - val_mean_loss: 69.7710 - val_std_loss: 17.2253\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 245.13512\n",
      "Epoch 19/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 248.8333 - sequence_loss: 156.9839 - mean_loss: 74.2164 - std_loss: 17.9009 - val_loss: 243.5985 - val_sequence_loss: 156.9109 - val_mean_loss: 69.0469 - val_std_loss: 17.3148\n",
      "\n",
      "Epoch 00019: val_loss improved from 245.13512 to 243.59845, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 20/300\n",
      "14228/14228 [==============================] - 20s 1ms/step - loss: 247.9554 - sequence_loss: 156.3638 - mean_loss: 73.6624 - std_loss: 17.7529 - val_loss: 255.1652 - val_sequence_loss: 161.7450 - val_mean_loss: 75.5964 - val_std_loss: 17.0653\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 243.59845\n",
      "Epoch 21/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 253.7981 - sequence_loss: 159.2097 - mean_loss: 76.7034 - std_loss: 17.7829 - val_loss: 244.1443 - val_sequence_loss: 157.4356 - val_mean_loss: 68.9701 - val_std_loss: 16.9921\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 243.59845\n",
      "Epoch 22/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 247.2106 - sequence_loss: 156.4756 - mean_loss: 73.1715 - std_loss: 17.6956 - val_loss: 242.3855 - val_sequence_loss: 156.5878 - val_mean_loss: 68.3488 - val_std_loss: 17.1188\n",
      "\n",
      "Epoch 00022: val_loss improved from 243.59845 to 242.38551, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 23/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 246.7888 - sequence_loss: 156.2357 - mean_loss: 72.6862 - std_loss: 17.6501 - val_loss: 241.7993 - val_sequence_loss: 156.2597 - val_mean_loss: 67.9133 - val_std_loss: 17.2549\n",
      "\n",
      "Epoch 00023: val_loss improved from 242.38551 to 241.79928, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 24/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14228/14228 [==============================] - 18s 1ms/step - loss: 246.1478 - sequence_loss: 155.9138 - mean_loss: 72.4069 - std_loss: 17.7727 - val_loss: 241.0851 - val_sequence_loss: 155.9550 - val_mean_loss: 67.6521 - val_std_loss: 17.2631\n",
      "\n",
      "Epoch 00024: val_loss improved from 241.79928 to 241.08510, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 25/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 245.2666 - sequence_loss: 155.6236 - mean_loss: 72.0059 - std_loss: 17.7100 - val_loss: 242.3769 - val_sequence_loss: 156.5247 - val_mean_loss: 68.0660 - val_std_loss: 17.2756\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 241.08510\n",
      "Epoch 26/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 247.4177 - sequence_loss: 156.6955 - mean_loss: 72.9023 - std_loss: 17.7799 - val_loss: 244.0180 - val_sequence_loss: 157.2073 - val_mean_loss: 68.9674 - val_std_loss: 17.3078\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 241.08510\n",
      "Epoch 27/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 246.8392 - sequence_loss: 156.2336 - mean_loss: 72.9100 - std_loss: 17.7869 - val_loss: 241.0118 - val_sequence_loss: 155.9582 - val_mean_loss: 67.4166 - val_std_loss: 17.1546\n",
      "\n",
      "Epoch 00027: val_loss improved from 241.08510 to 241.01182, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 28/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 245.7501 - sequence_loss: 155.9057 - mean_loss: 72.1420 - std_loss: 17.8101 - val_loss: 241.1757 - val_sequence_loss: 156.2289 - val_mean_loss: 67.4305 - val_std_loss: 17.1013\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 241.01182\n",
      "Epoch 29/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 245.0090 - sequence_loss: 155.4966 - mean_loss: 71.8102 - std_loss: 17.7953 - val_loss: 239.6550 - val_sequence_loss: 155.4741 - val_mean_loss: 66.4767 - val_std_loss: 17.0891\n",
      "\n",
      "Epoch 00029: val_loss improved from 241.01182 to 239.65497, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 30/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 244.4703 - sequence_loss: 155.2921 - mean_loss: 71.3488 - std_loss: 17.8216 - val_loss: 240.6808 - val_sequence_loss: 155.7420 - val_mean_loss: 67.2883 - val_std_loss: 17.2912\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 239.65497\n",
      "Epoch 31/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 244.6889 - sequence_loss: 155.4182 - mean_loss: 71.3332 - std_loss: 17.7955 - val_loss: 239.4141 - val_sequence_loss: 155.5782 - val_mean_loss: 66.1608 - val_std_loss: 17.3168\n",
      "\n",
      "Epoch 00031: val_loss improved from 239.65497 to 239.41412, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 32/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.9263 - sequence_loss: 155.1079 - mean_loss: 71.0194 - std_loss: 17.7254 - val_loss: 239.6504 - val_sequence_loss: 155.7740 - val_mean_loss: 66.0597 - val_std_loss: 17.1927\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 239.41412\n",
      "Epoch 33/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 244.3730 - sequence_loss: 155.5636 - mean_loss: 71.1311 - std_loss: 17.7255 - val_loss: 238.8878 - val_sequence_loss: 155.5258 - val_mean_loss: 65.9392 - val_std_loss: 17.1478\n",
      "\n",
      "Epoch 00033: val_loss improved from 239.41412 to 238.88779, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 34/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 245.3754 - sequence_loss: 155.8519 - mean_loss: 71.7434 - std_loss: 17.8191 - val_loss: 240.7012 - val_sequence_loss: 155.8180 - val_mean_loss: 67.6191 - val_std_loss: 17.2611\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 238.88779\n",
      "Epoch 35/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.8471 - sequence_loss: 155.3517 - mean_loss: 70.8705 - std_loss: 17.6718 - val_loss: 239.5455 - val_sequence_loss: 155.8364 - val_mean_loss: 66.7485 - val_std_loss: 17.1813\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 238.88779\n",
      "Epoch 36/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.1437 - sequence_loss: 155.3439 - mean_loss: 70.3050 - std_loss: 17.7136 - val_loss: 238.1430 - val_sequence_loss: 155.0978 - val_mean_loss: 65.7289 - val_std_loss: 17.1709\n",
      "\n",
      "Epoch 00036: val_loss improved from 238.88779 to 238.14300, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 37/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 245.1326 - sequence_loss: 156.0663 - mean_loss: 71.2382 - std_loss: 17.6843 - val_loss: 241.7732 - val_sequence_loss: 156.8637 - val_mean_loss: 66.9180 - val_std_loss: 17.4004\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 238.14300\n",
      "Epoch 38/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 245.0184 - sequence_loss: 155.6726 - mean_loss: 71.6260 - std_loss: 17.8188 - val_loss: 238.6818 - val_sequence_loss: 155.2346 - val_mean_loss: 66.2014 - val_std_loss: 17.1453\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 238.14300\n",
      "Epoch 39/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 244.5581 - sequence_loss: 155.4693 - mean_loss: 71.3960 - std_loss: 17.7644 - val_loss: 244.7541 - val_sequence_loss: 156.8221 - val_mean_loss: 70.0112 - val_std_loss: 17.2673\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 238.14300\n",
      "Epoch 40/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.6565 - sequence_loss: 155.0867 - mean_loss: 70.7001 - std_loss: 17.6996 - val_loss: 238.7472 - val_sequence_loss: 155.3690 - val_mean_loss: 65.8651 - val_std_loss: 17.1761\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 238.14300\n",
      "Epoch 41/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.9601 - sequence_loss: 155.3735 - mean_loss: 70.6441 - std_loss: 17.8044 - val_loss: 239.9073 - val_sequence_loss: 156.0199 - val_mean_loss: 65.9781 - val_std_loss: 17.1166\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 238.14300\n",
      "Epoch 42/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.9634 - sequence_loss: 154.9090 - mean_loss: 70.5243 - std_loss: 17.7295 - val_loss: 239.1281 - val_sequence_loss: 155.4052 - val_mean_loss: 66.0613 - val_std_loss: 17.2281\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 238.14300\n",
      "Epoch 43/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.6831 - sequence_loss: 155.4002 - mean_loss: 70.6833 - std_loss: 17.7399 - val_loss: 239.5843 - val_sequence_loss: 155.8317 - val_mean_loss: 65.9926 - val_std_loss: 17.1156\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 238.14300\n",
      "Epoch 44/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.6190 - sequence_loss: 154.8259 - mean_loss: 69.9365 - std_loss: 17.7145 - val_loss: 238.5615 - val_sequence_loss: 155.0672 - val_mean_loss: 66.0719 - val_std_loss: 16.9704\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 238.14300\n",
      "Epoch 45/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.5463 - sequence_loss: 155.2944 - mean_loss: 70.2312 - std_loss: 17.8188 - val_loss: 238.2280 - val_sequence_loss: 155.2174 - val_mean_loss: 65.6826 - val_std_loss: 17.2353\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 238.14300\n",
      "Epoch 46/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.8373 - sequence_loss: 155.0234 - mean_loss: 69.8459 - std_loss: 17.8356 - val_loss: 238.3458 - val_sequence_loss: 155.1820 - val_mean_loss: 65.5540 - val_std_loss: 17.3407\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 238.14300\n",
      "Epoch 47/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.3107 - sequence_loss: 154.8315 - mean_loss: 69.9145 - std_loss: 17.8112 - val_loss: 238.0851 - val_sequence_loss: 155.0715 - val_mean_loss: 65.2378 - val_std_loss: 17.2964\n",
      "\n",
      "Epoch 00047: val_loss improved from 238.14300 to 238.08510, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 48/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.2544 - sequence_loss: 155.3947 - mean_loss: 70.3439 - std_loss: 17.7559 - val_loss: 243.1285 - val_sequence_loss: 157.4720 - val_mean_loss: 67.7845 - val_std_loss: 17.4818\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 238.08510\n",
      "Epoch 49/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14228/14228 [==============================] - 18s 1ms/step - loss: 244.2159 - sequence_loss: 155.6568 - mean_loss: 70.9856 - std_loss: 17.7007 - val_loss: 238.6580 - val_sequence_loss: 155.3829 - val_mean_loss: 66.2438 - val_std_loss: 17.0728\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 238.08510\n",
      "Epoch 50/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.8174 - sequence_loss: 154.9261 - mean_loss: 70.4241 - std_loss: 17.7471 - val_loss: 238.1495 - val_sequence_loss: 155.0252 - val_mean_loss: 65.8114 - val_std_loss: 17.1705\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 238.08510\n",
      "Epoch 51/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.0873 - sequence_loss: 154.7169 - mean_loss: 70.1370 - std_loss: 17.7229 - val_loss: 238.8710 - val_sequence_loss: 155.0242 - val_mean_loss: 66.3753 - val_std_loss: 17.2598\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 238.08510\n",
      "Epoch 52/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 245.2906 - sequence_loss: 155.9032 - mean_loss: 71.2997 - std_loss: 17.7211 - val_loss: 240.2057 - val_sequence_loss: 155.6703 - val_mean_loss: 66.9086 - val_std_loss: 17.2988\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 238.08510\n",
      "Epoch 53/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.0326 - sequence_loss: 155.2179 - mean_loss: 70.0457 - std_loss: 17.6637 - val_loss: 239.0937 - val_sequence_loss: 155.7024 - val_mean_loss: 66.1511 - val_std_loss: 17.0237\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 238.08510\n",
      "Epoch 54/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.4770 - sequence_loss: 154.5011 - mean_loss: 69.2660 - std_loss: 17.6811 - val_loss: 238.4068 - val_sequence_loss: 155.0192 - val_mean_loss: 65.8156 - val_std_loss: 17.2789\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 238.08510\n",
      "Epoch 55/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.8451 - sequence_loss: 154.6755 - mean_loss: 69.1376 - std_loss: 17.7364 - val_loss: 238.1667 - val_sequence_loss: 155.1310 - val_mean_loss: 65.6482 - val_std_loss: 17.2709\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 238.08510\n",
      "Epoch 56/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.4618 - sequence_loss: 154.5992 - mean_loss: 69.1001 - std_loss: 17.6527 - val_loss: 237.3340 - val_sequence_loss: 154.7565 - val_mean_loss: 65.1225 - val_std_loss: 17.1582\n",
      "\n",
      "Epoch 00056: val_loss improved from 238.08510 to 237.33403, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 57/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.3700 - sequence_loss: 154.6870 - mean_loss: 69.0436 - std_loss: 17.6314 - val_loss: 246.1655 - val_sequence_loss: 158.3553 - val_mean_loss: 70.6177 - val_std_loss: 17.1966\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 237.33403\n",
      "Epoch 58/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.8060 - sequence_loss: 155.3836 - mean_loss: 70.0545 - std_loss: 17.5485 - val_loss: 239.0252 - val_sequence_loss: 155.4954 - val_mean_loss: 65.9724 - val_std_loss: 17.2416\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 237.33403\n",
      "Epoch 59/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.8115 - sequence_loss: 154.7081 - mean_loss: 69.2008 - std_loss: 17.5825 - val_loss: 236.9363 - val_sequence_loss: 154.5698 - val_mean_loss: 64.8442 - val_std_loss: 17.2228\n",
      "\n",
      "Epoch 00059: val_loss improved from 237.33403 to 236.93630, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 60/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 240.7456 - sequence_loss: 154.4048 - mean_loss: 68.6101 - std_loss: 17.6347 - val_loss: 237.3482 - val_sequence_loss: 154.7111 - val_mean_loss: 65.3391 - val_std_loss: 17.0214\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 236.93630\n",
      "Epoch 61/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.3414 - sequence_loss: 154.7248 - mean_loss: 69.0970 - std_loss: 17.7270 - val_loss: 239.7115 - val_sequence_loss: 155.5132 - val_mean_loss: 66.5003 - val_std_loss: 17.1020\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 236.93630\n",
      "Epoch 62/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.1920 - sequence_loss: 154.6650 - mean_loss: 69.0155 - std_loss: 17.6642 - val_loss: 238.3430 - val_sequence_loss: 155.0875 - val_mean_loss: 65.3155 - val_std_loss: 17.3235\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 236.93630\n",
      "Epoch 63/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.2126 - sequence_loss: 154.3816 - mean_loss: 69.1823 - std_loss: 17.7343 - val_loss: 237.5730 - val_sequence_loss: 154.6991 - val_mean_loss: 65.4149 - val_std_loss: 17.2769\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 236.93630\n",
      "Epoch 64/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 240.7130 - sequence_loss: 154.2050 - mean_loss: 69.1511 - std_loss: 17.7056 - val_loss: 243.2714 - val_sequence_loss: 157.1393 - val_mean_loss: 67.8871 - val_std_loss: 17.2938\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 236.93630\n",
      "Epoch 65/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.6421 - sequence_loss: 154.6063 - mean_loss: 69.4015 - std_loss: 17.6988 - val_loss: 242.9514 - val_sequence_loss: 156.9791 - val_mean_loss: 67.6006 - val_std_loss: 17.7613\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 236.93630\n",
      "Epoch 66/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 262.5074 - sequence_loss: 163.0680 - mean_loss: 80.9197 - std_loss: 17.8369 - val_loss: 241.5127 - val_sequence_loss: 156.1910 - val_mean_loss: 67.7752 - val_std_loss: 17.2783\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 236.93630\n",
      "Epoch 67/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 245.9138 - sequence_loss: 155.9095 - mean_loss: 71.8052 - std_loss: 17.6827 - val_loss: 241.0063 - val_sequence_loss: 156.2123 - val_mean_loss: 67.5278 - val_std_loss: 17.1078\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 236.93630\n",
      "Epoch 68/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 244.2591 - sequence_loss: 155.7097 - mean_loss: 70.8374 - std_loss: 17.7670 - val_loss: 241.1241 - val_sequence_loss: 156.3865 - val_mean_loss: 67.2645 - val_std_loss: 17.4911\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 236.93630\n",
      "Epoch 69/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 245.0390 - sequence_loss: 156.1955 - mean_loss: 70.8312 - std_loss: 17.8992 - val_loss: 240.5304 - val_sequence_loss: 156.3475 - val_mean_loss: 67.1020 - val_std_loss: 17.0605\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 236.93630\n",
      "Epoch 70/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.6724 - sequence_loss: 155.1226 - mean_loss: 70.5865 - std_loss: 17.7273 - val_loss: 239.0775 - val_sequence_loss: 155.3844 - val_mean_loss: 66.3807 - val_std_loss: 17.4024\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 236.93630\n",
      "Epoch 71/300\n",
      "14228/14228 [==============================] - 19s 1ms/step - loss: 243.2638 - sequence_loss: 155.3187 - mean_loss: 70.2927 - std_loss: 17.7613 - val_loss: 239.8374 - val_sequence_loss: 155.3723 - val_mean_loss: 66.9542 - val_std_loss: 17.2779\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 236.93630\n",
      "Epoch 72/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.7887 - sequence_loss: 154.9014 - mean_loss: 70.2079 - std_loss: 17.6700 - val_loss: 239.7903 - val_sequence_loss: 155.3131 - val_mean_loss: 67.3841 - val_std_loss: 17.2931\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 236.93630\n",
      "Epoch 73/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.6216 - sequence_loss: 154.7582 - mean_loss: 70.0721 - std_loss: 17.6548 - val_loss: 238.9129 - val_sequence_loss: 155.0053 - val_mean_loss: 66.2635 - val_std_loss: 17.3312\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 236.93630\n",
      "Epoch 74/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.3856 - sequence_loss: 154.8602 - mean_loss: 69.8895 - std_loss: 17.6994 - val_loss: 238.6052 - val_sequence_loss: 154.9666 - val_mean_loss: 65.9876 - val_std_loss: 17.1876\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 236.93630\n",
      "Epoch 75/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.2664 - sequence_loss: 154.7403 - mean_loss: 70.0082 - std_loss: 17.7279 - val_loss: 238.9082 - val_sequence_loss: 155.2435 - val_mean_loss: 66.1023 - val_std_loss: 17.2921\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 236.93630\n",
      "Epoch 76/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.7133 - sequence_loss: 154.3310 - mean_loss: 69.5013 - std_loss: 17.7552 - val_loss: 238.8517 - val_sequence_loss: 155.1463 - val_mean_loss: 66.4948 - val_std_loss: 17.2874\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 236.93630\n",
      "Epoch 77/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.9795 - sequence_loss: 154.6412 - mean_loss: 69.4603 - std_loss: 17.7518 - val_loss: 238.5456 - val_sequence_loss: 154.7374 - val_mean_loss: 66.5819 - val_std_loss: 17.2042\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 236.93630\n",
      "Epoch 78/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.1619 - sequence_loss: 154.5689 - mean_loss: 69.8538 - std_loss: 17.8209 - val_loss: 237.3257 - val_sequence_loss: 154.5451 - val_mean_loss: 65.0603 - val_std_loss: 17.5259\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 236.93630\n",
      "Epoch 79/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.0428 - sequence_loss: 154.5289 - mean_loss: 69.5878 - std_loss: 17.7544 - val_loss: 242.2818 - val_sequence_loss: 156.1959 - val_mean_loss: 68.9559 - val_std_loss: 17.4666\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 236.93630\n",
      "Epoch 80/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 242.6329 - sequence_loss: 154.7664 - mean_loss: 70.0802 - std_loss: 17.8307 - val_loss: 237.9054 - val_sequence_loss: 154.6956 - val_mean_loss: 65.2627 - val_std_loss: 17.4463\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 236.93630\n",
      "Epoch 81/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.5720 - sequence_loss: 154.3701 - mean_loss: 69.0743 - std_loss: 17.8212 - val_loss: 237.0722 - val_sequence_loss: 154.5247 - val_mean_loss: 65.0454 - val_std_loss: 17.3837\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 236.93630\n",
      "Epoch 82/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.0377 - sequence_loss: 154.1924 - mean_loss: 68.8635 - std_loss: 17.9893 - val_loss: 237.0456 - val_sequence_loss: 154.3156 - val_mean_loss: 64.9952 - val_std_loss: 17.5251\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 236.93630\n",
      "Epoch 83/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 240.8442 - sequence_loss: 153.9919 - mean_loss: 68.6385 - std_loss: 17.8918 - val_loss: 237.5031 - val_sequence_loss: 154.6941 - val_mean_loss: 65.5885 - val_std_loss: 17.5605\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 236.93630\n",
      "Epoch 84/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.1698 - sequence_loss: 154.2340 - mean_loss: 69.0106 - std_loss: 17.9698 - val_loss: 236.7847 - val_sequence_loss: 154.2021 - val_mean_loss: 64.8063 - val_std_loss: 17.4298\n",
      "\n",
      "Epoch 00084: val_loss improved from 236.93630 to 236.78471, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 85/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 241.0721 - sequence_loss: 153.9881 - mean_loss: 69.1064 - std_loss: 17.9385 - val_loss: 237.4296 - val_sequence_loss: 154.1711 - val_mean_loss: 65.5789 - val_std_loss: 17.5416\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 236.78471\n",
      "Epoch 86/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 240.8346 - sequence_loss: 154.0654 - mean_loss: 68.7293 - std_loss: 17.9218 - val_loss: 237.1605 - val_sequence_loss: 154.0356 - val_mean_loss: 65.5899 - val_std_loss: 17.5978\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 236.78471\n",
      "Epoch 87/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 240.9543 - sequence_loss: 154.0329 - mean_loss: 68.9555 - std_loss: 18.0210 - val_loss: 238.2492 - val_sequence_loss: 154.4967 - val_mean_loss: 66.1165 - val_std_loss: 17.6514\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 236.78471\n",
      "Epoch 88/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 240.5421 - sequence_loss: 153.7971 - mean_loss: 68.5845 - std_loss: 18.0473 - val_loss: 238.3307 - val_sequence_loss: 154.5451 - val_mean_loss: 65.7561 - val_std_loss: 17.6007\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 236.78471\n",
      "Epoch 89/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 240.6103 - sequence_loss: 153.9082 - mean_loss: 68.5253 - std_loss: 18.0819 - val_loss: 237.2758 - val_sequence_loss: 154.2953 - val_mean_loss: 65.2326 - val_std_loss: 17.8928\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 236.78471\n",
      "Epoch 90/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 239.8763 - sequence_loss: 153.4426 - mean_loss: 68.0869 - std_loss: 18.0398 - val_loss: 235.7520 - val_sequence_loss: 153.7409 - val_mean_loss: 64.3439 - val_std_loss: 17.5502\n",
      "\n",
      "Epoch 00090: val_loss improved from 236.78471 to 235.75197, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 91/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 239.7754 - sequence_loss: 153.4248 - mean_loss: 68.2525 - std_loss: 18.0556 - val_loss: 235.7022 - val_sequence_loss: 154.0536 - val_mean_loss: 63.6570 - val_std_loss: 17.8885\n",
      "\n",
      "Epoch 00091: val_loss improved from 235.75197 to 235.70224, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 92/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 239.9230 - sequence_loss: 153.4650 - mean_loss: 68.3799 - std_loss: 18.2081 - val_loss: 235.4628 - val_sequence_loss: 153.3518 - val_mean_loss: 64.2441 - val_std_loss: 17.6214\n",
      "\n",
      "Epoch 00092: val_loss improved from 235.70224 to 235.46275, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 93/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 239.5397 - sequence_loss: 153.1093 - mean_loss: 68.0397 - std_loss: 18.1416 - val_loss: 239.7905 - val_sequence_loss: 154.8369 - val_mean_loss: 67.2722 - val_std_loss: 17.8721\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 235.46275\n",
      "Epoch 94/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 240.3505 - sequence_loss: 153.4275 - mean_loss: 68.6234 - std_loss: 18.2897 - val_loss: 235.7519 - val_sequence_loss: 153.3780 - val_mean_loss: 64.3778 - val_std_loss: 18.0552\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 235.46275\n",
      "Epoch 95/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 238.8463 - sequence_loss: 152.5594 - mean_loss: 67.7832 - std_loss: 18.5594 - val_loss: 234.5702 - val_sequence_loss: 152.6589 - val_mean_loss: 63.4181 - val_std_loss: 18.2727\n",
      "\n",
      "Epoch 00095: val_loss improved from 235.46275 to 234.57016, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 96/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 238.6679 - sequence_loss: 151.8203 - mean_loss: 67.7674 - std_loss: 18.7095 - val_loss: 234.8742 - val_sequence_loss: 151.8459 - val_mean_loss: 64.0873 - val_std_loss: 18.6648\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 234.57016\n",
      "Epoch 97/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 238.7168 - sequence_loss: 151.4339 - mean_loss: 68.1837 - std_loss: 19.0876 - val_loss: 234.8576 - val_sequence_loss: 151.7827 - val_mean_loss: 64.0152 - val_std_loss: 18.6170\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 234.57016\n",
      "Epoch 98/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 237.2621 - sequence_loss: 150.8033 - mean_loss: 67.2548 - std_loss: 19.4053 - val_loss: 233.8527 - val_sequence_loss: 151.0573 - val_mean_loss: 63.4964 - val_std_loss: 18.9714\n",
      "\n",
      "Epoch 00098: val_loss improved from 234.57016 to 233.85271, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 99/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 238.3006 - sequence_loss: 151.2085 - mean_loss: 67.8410 - std_loss: 19.4766 - val_loss: 232.6770 - val_sequence_loss: 150.4781 - val_mean_loss: 62.9752 - val_std_loss: 18.8982\n",
      "\n",
      "Epoch 00099: val_loss improved from 233.85271 to 232.67703, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 100/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14228/14228 [==============================] - 18s 1ms/step - loss: 236.9312 - sequence_loss: 149.5914 - mean_loss: 67.9112 - std_loss: 19.5902 - val_loss: 235.7628 - val_sequence_loss: 151.5897 - val_mean_loss: 65.3897 - val_std_loss: 18.5303\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 232.67703\n",
      "Epoch 101/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 235.9159 - sequence_loss: 148.9803 - mean_loss: 67.7554 - std_loss: 19.0832 - val_loss: 227.9274 - val_sequence_loss: 147.4551 - val_mean_loss: 62.5682 - val_std_loss: 18.0150\n",
      "\n",
      "Epoch 00101: val_loss improved from 232.67703 to 227.92741, saving model to ../models/base_LSTM.hdf5\n",
      "Epoch 102/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 240.7424 - sequence_loss: 151.5777 - mean_loss: 68.3379 - std_loss: 20.5746 - val_loss: 235.5527 - val_sequence_loss: 152.3675 - val_mean_loss: 64.0438 - val_std_loss: 18.5699\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 227.92741\n",
      "Epoch 103/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 238.0236 - sequence_loss: 151.4158 - mean_loss: 67.3338 - std_loss: 18.9707 - val_loss: 233.6292 - val_sequence_loss: 151.3027 - val_mean_loss: 63.5435 - val_std_loss: 18.7174\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 227.92741\n",
      "Epoch 104/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 237.4550 - sequence_loss: 150.8283 - mean_loss: 67.3915 - std_loss: 19.1117 - val_loss: 232.3014 - val_sequence_loss: 150.8672 - val_mean_loss: 62.4967 - val_std_loss: 18.4801\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 227.92741\n",
      "Epoch 105/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 235.4205 - sequence_loss: 148.8889 - mean_loss: 66.9698 - std_loss: 19.4262 - val_loss: 230.9675 - val_sequence_loss: 147.7491 - val_mean_loss: 63.7825 - val_std_loss: 19.6287\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 227.92741\n",
      "Epoch 106/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 232.5234 - sequence_loss: 146.9917 - mean_loss: 66.4645 - std_loss: 19.1478 - val_loss: 228.5295 - val_sequence_loss: 146.7065 - val_mean_loss: 63.2597 - val_std_loss: 18.3891\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 227.92741\n",
      "Epoch 107/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 245.1173 - sequence_loss: 152.6377 - mean_loss: 69.1632 - std_loss: 23.3526 - val_loss: 243.6354 - val_sequence_loss: 156.5116 - val_mean_loss: 66.1717 - val_std_loss: 21.1057\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 227.92741\n",
      "Epoch 108/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 243.2239 - sequence_loss: 154.9321 - mean_loss: 69.4613 - std_loss: 18.7777 - val_loss: 236.2900 - val_sequence_loss: 153.7936 - val_mean_loss: 64.6069 - val_std_loss: 17.7757\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 227.92741\n",
      "Epoch 109/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 240.0282 - sequence_loss: 153.4264 - mean_loss: 68.4242 - std_loss: 18.0164 - val_loss: 235.8115 - val_sequence_loss: 153.7918 - val_mean_loss: 63.9438 - val_std_loss: 17.6680\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 227.92741\n",
      "Epoch 110/300\n",
      "14228/14228 [==============================] - 19s 1ms/step - loss: 239.4562 - sequence_loss: 153.5301 - mean_loss: 67.9832 - std_loss: 18.0590 - val_loss: 235.2284 - val_sequence_loss: 153.5876 - val_mean_loss: 63.5647 - val_std_loss: 17.5512\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 227.92741\n",
      "Epoch 111/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 239.0248 - sequence_loss: 153.3204 - mean_loss: 67.7967 - std_loss: 18.0906 - val_loss: 234.8597 - val_sequence_loss: 153.2995 - val_mean_loss: 63.7482 - val_std_loss: 17.7428\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 227.92741\n",
      "Epoch 112/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 239.1119 - sequence_loss: 152.9982 - mean_loss: 67.7977 - std_loss: 18.0604 - val_loss: 234.5600 - val_sequence_loss: 153.1868 - val_mean_loss: 63.4157 - val_std_loss: 17.7504\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 227.92741\n",
      "Epoch 113/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 238.8139 - sequence_loss: 153.1116 - mean_loss: 67.6441 - std_loss: 18.1344 - val_loss: 238.2787 - val_sequence_loss: 154.3510 - val_mean_loss: 65.5655 - val_std_loss: 17.7528\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 227.92741\n",
      "Epoch 114/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 239.1668 - sequence_loss: 153.0480 - mean_loss: 67.7217 - std_loss: 18.0636 - val_loss: 236.3519 - val_sequence_loss: 153.4066 - val_mean_loss: 65.5262 - val_std_loss: 17.8832\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 227.92741\n",
      "Epoch 115/300\n",
      "14228/14228 [==============================] - 18s 1ms/step - loss: 239.3733 - sequence_loss: 153.0896 - mean_loss: 67.8761 - std_loss: 18.2104 - val_loss: 235.0791 - val_sequence_loss: 153.3174 - val_mean_loss: 63.6986 - val_std_loss: 17.6722\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 227.92741\n",
      "Epoch 116/300\n",
      "14228/14228 [==============================] - 9s 616us/step - loss: 238.9359 - sequence_loss: 152.8037 - mean_loss: 68.1150 - std_loss: 18.1156 - val_loss: 236.2018 - val_sequence_loss: 153.7308 - val_mean_loss: 65.3531 - val_std_loss: 17.7934\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 227.92741\n",
      "Epoch 117/300\n",
      "14228/14228 [==============================] - 9s 616us/step - loss: 238.7474 - sequence_loss: 152.8194 - mean_loss: 67.5703 - std_loss: 18.2321 - val_loss: 234.5501 - val_sequence_loss: 152.7841 - val_mean_loss: 63.6891 - val_std_loss: 17.8913\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 227.92741\n",
      "Epoch 118/300\n",
      "14228/14228 [==============================] - 9s 614us/step - loss: 238.2043 - sequence_loss: 152.5788 - mean_loss: 67.4939 - std_loss: 18.2071 - val_loss: 235.1373 - val_sequence_loss: 153.4217 - val_mean_loss: 63.5081 - val_std_loss: 17.6792\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 227.92741\n",
      "Epoch 119/300\n",
      "14228/14228 [==============================] - 9s 616us/step - loss: 239.5672 - sequence_loss: 153.2979 - mean_loss: 68.0266 - std_loss: 18.1661 - val_loss: 235.1301 - val_sequence_loss: 153.2426 - val_mean_loss: 63.9229 - val_std_loss: 17.8188\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 227.92741\n",
      "Epoch 120/300\n",
      "14228/14228 [==============================] - 9s 615us/step - loss: 238.2748 - sequence_loss: 152.7978 - mean_loss: 67.2017 - std_loss: 18.2075 - val_loss: 237.8681 - val_sequence_loss: 154.0337 - val_mean_loss: 66.5435 - val_std_loss: 17.8397\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 227.92741\n",
      "Epoch 121/300\n",
      "14228/14228 [==============================] - 9s 614us/step - loss: 238.6602 - sequence_loss: 153.0514 - mean_loss: 67.3916 - std_loss: 18.1885 - val_loss: 233.7277 - val_sequence_loss: 152.9881 - val_mean_loss: 62.9058 - val_std_loss: 17.7901\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 227.92741\n",
      "Epoch 122/300\n",
      "14228/14228 [==============================] - 9s 614us/step - loss: 237.7048 - sequence_loss: 152.6554 - mean_loss: 66.8837 - std_loss: 18.2393 - val_loss: 235.3806 - val_sequence_loss: 153.3892 - val_mean_loss: 64.2305 - val_std_loss: 17.9263\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 227.92741\n",
      "Epoch 123/300\n",
      "14228/14228 [==============================] - 9s 615us/step - loss: 238.7955 - sequence_loss: 152.7791 - mean_loss: 67.6273 - std_loss: 18.2212 - val_loss: 233.3346 - val_sequence_loss: 152.5352 - val_mean_loss: 62.4562 - val_std_loss: 18.0596\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 227.92741\n",
      "Epoch 124/300\n",
      "14228/14228 [==============================] - 9s 614us/step - loss: 237.3943 - sequence_loss: 152.3900 - mean_loss: 66.6673 - std_loss: 18.3619 - val_loss: 234.0255 - val_sequence_loss: 152.7027 - val_mean_loss: 63.1019 - val_std_loss: 18.1075\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 227.92741\n",
      "Epoch 125/300\n",
      "14228/14228 [==============================] - 9s 615us/step - loss: 237.6865 - sequence_loss: 152.5466 - mean_loss: 66.7792 - std_loss: 18.2404 - val_loss: 234.0034 - val_sequence_loss: 153.2945 - val_mean_loss: 62.4859 - val_std_loss: 18.0518\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 227.92741\n",
      "Epoch 126/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14228/14228 [==============================] - 9s 613us/step - loss: 274.6547 - sequence_loss: 168.8504 - mean_loss: 87.0742 - std_loss: 18.6609 - val_loss: 260.1796 - val_sequence_loss: 167.2428 - val_mean_loss: 74.6496 - val_std_loss: 17.9228\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 227.92741\n",
      "Epoch 127/300\n",
      "14228/14228 [==============================] - 9s 618us/step - loss: 258.6902 - sequence_loss: 163.3105 - mean_loss: 76.9824 - std_loss: 18.2441 - val_loss: 253.0420 - val_sequence_loss: 161.8730 - val_mean_loss: 73.0708 - val_std_loss: 17.6565\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 227.92741\n",
      "Epoch 128/300\n",
      "14228/14228 [==============================] - 9s 612us/step - loss: 255.4740 - sequence_loss: 162.4109 - mean_loss: 74.8945 - std_loss: 18.0756 - val_loss: 255.8814 - val_sequence_loss: 163.8774 - val_mean_loss: 74.0018 - val_std_loss: 17.7912\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 227.92741\n",
      "Epoch 129/300\n",
      "14228/14228 [==============================] - 9s 613us/step - loss: 251.4802 - sequence_loss: 161.0245 - mean_loss: 72.5125 - std_loss: 18.0102 - val_loss: 243.3130 - val_sequence_loss: 159.5260 - val_mean_loss: 66.4507 - val_std_loss: 17.3479\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 227.92741\n",
      "Epoch 130/300\n",
      "14228/14228 [==============================] - 9s 612us/step - loss: 247.4577 - sequence_loss: 159.7645 - mean_loss: 70.2382 - std_loss: 17.7886 - val_loss: 242.2721 - val_sequence_loss: 159.8757 - val_mean_loss: 64.9125 - val_std_loss: 17.4065\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 227.92741\n",
      "Epoch 131/300\n",
      "14228/14228 [==============================] - 9s 613us/step - loss: 247.6583 - sequence_loss: 159.9565 - mean_loss: 69.8187 - std_loss: 17.7084 - val_loss: 241.7741 - val_sequence_loss: 159.5059 - val_mean_loss: 64.4548 - val_std_loss: 17.6755\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 227.92741\n",
      "Epoch 00131: early stopping\n"
     ]
    }
   ],
   "source": [
    "filepath = '../models/base_LSTM.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=30)\n",
    "callbacks_list = [es,checkpoint]\n",
    "history = sequence_autoencoder.fit(X_train, [y_train,means_train,stds_train],\n",
    "                epochs=300,\n",
    "                batch_size=500,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_val,[y_val,means_val,stds_val]),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "sequence_autoencoder = load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = sequence_autoencoder.predict(X_test)\n",
    "y_pred = y_pred1[0]\n",
    "mean_pred = y_pred1[1]\n",
    "stds_pred = y_pred1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1[0].shape,y_pred1[1].shape,y_pred1[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib  inline\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.plot(y[-10])\n",
    "for i,a in enumerate(y_pred[::-1][20:100]):\n",
    "#     if np.sum(a)>0:\n",
    "    plt.figure()\n",
    "    plt.plot(a,'g')\n",
    "#         plt.plot(X[i,:,:],'r')\n",
    "#     plt.plot(y_test[i],'b')\n",
    "    plt.plot(ecg_test[i],'r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(quals_test.reshape(-1)),4))\n",
    "X[:,0] = quals_test.reshape(-1)\n",
    "X[:,1] = y_pred.reshape(-1)\n",
    "X[:,2] = ecg_test.reshape(-1)\n",
    "X[:,3] = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[X[:,0]>-1]\n",
    "X = X[X[:,2]>0]\n",
    "X = X[X[:,3]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_range = np.arange(0,1,.05)\n",
    "x = []\n",
    "y = []\n",
    "y1 = []\n",
    "for l in l_range:\n",
    "    index = np.where((X[:,0]>=l)&(X[:,0]<l+.05))[0]\n",
    "    temp = X[index]\n",
    "    x.append(str(np.round(l*100)/100)+'-'+str(np.round((l+.05)*100)/100))\n",
    "    y.append(list(np.abs(temp[:,2]-temp[:,3])))\n",
    "    y1.append(list(np.abs(temp[:,1]-temp[:,2])))\n",
    "#     print(np.mean(np.abs(temp[:,0]-temp[:,2])),np.std(np.abs(temp[:,0]-temp[:,2])),len(index))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.rcParams.update({'font.size':20})\n",
    "c = plt.boxplot(y,showfliers=False,positions=np.array(range(0,3*len(y),3)),notch=True)\n",
    "for box in c['boxes']:\n",
    "    box.set(color='red', linewidth=1)\n",
    "b = plt.boxplot(y1,showfliers=False,positions=np.array(range(0,3*len(y),3))+1.5,notch=True)\n",
    "for box in b['boxes']:\n",
    "    box.set(color='blue', linewidth=1)\n",
    "#     box.set(facecolor = 'red' )\n",
    "plt.xticks(np.array(range(0,3*len(y),3)),x,rotation=60)\n",
    "plt.ylabel('Absolute Difference in Milliseconds')\n",
    "plt.xlabel('Range of Signal Quality')\n",
    "plt.tight_layout()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hrvanalysis import get_time_domain_features\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "q = []\n",
    "s = 'std_hr'\n",
    "for i in range(ecg_test.shape[0]):\n",
    "    qual_min = quals_test[i].reshape(-1)\n",
    "    qual_min = qual_min[qual_min>-1]\n",
    "    ecg_min = ecg_test[i].reshape(-1)\n",
    "    ecg_min = ecg_min[ecg_min>0]\n",
    "    ecg_min= ecg_min[~np.isnan(ecg_min)]\n",
    "    y_pred_min = y_pred[i].reshape(-1)\n",
    "    y_pred_min = y_pred_min[~np.isnan(y_pred_min)]\n",
    "    y_test_min = y_test[i].reshape(-1)\n",
    "    y_test_min = y_test_min[y_test_min>0]\n",
    "    y_test_min = y_test_min[~np.isnan(y_test_min)]\n",
    "    \n",
    "    x.append(get_time_domain_features(y_pred_min)[s])\n",
    "    y.append(get_time_domain_features(ecg_min)[s])\n",
    "    z.append(get_time_domain_features(y_test_min)[s])\n",
    "    q.append(np.median(qual_min))\n",
    "    if np.isinf(x[-1]) or np.isinf(y[-1]) or np.isinf(z[-1]) or np.isinf(q[-1]):\n",
    "        x = x[:-1]\n",
    "        y= y[:-1]\n",
    "        z= z[:-1]\n",
    "        q= q[:-1]\n",
    "    elif np.isnan(x[-1]) or np.isnan(y[-1]) or np.isnan(z[-1]) or np.isnan(q[-1]):\n",
    "        x = x[:-1]\n",
    "        y= y[:-1]\n",
    "        z= z[:-1]\n",
    "        q= q[:-1]\n",
    "#     print(np.std(ecg_min),np.std(y_test_min),np.std(y_pred_min),np.median(qual_min))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr,spearmanr\n",
    "q,x,y,z = np.array(q),np.array(x),np.array(y),np.array(z)\n",
    "for i in np.linspace(0,1,11):\n",
    "    index = np.where((q>=i)&(q<i+.1))[0]\n",
    "    if len(index)<2:\n",
    "        continue\n",
    "    print(pearsonr(x[index],y[index]),pearsonr(y[index],z[index]),i,len(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.hist(y.reshape(-1),50)\n",
    "plt.hist(y_pred.reshape(-1),50)\n",
    "plt.hist(ecg_test.reshape(-1),50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(y[0].reshape(1,-1,1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred1[1].reshape(-1),means_test.reshape(-1),'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(y_pred1[1].reshape(-1),means_test.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = y_pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[np.isfinite(t)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
